@BOOK{Chicone2006,
  title     = "Ordinary Differential Equations with Applications",
  author    = "Chicone, Carmen",
  abstract  = "Mathematics is playing an ever more important role in the
               physical and biological sciences, provoking a blurring of
               boundaries between scienti?c disciplines and a resurgence of
               interest in the modern as well as the cl- sical techniques of
               applied mathematics. This renewal of interest, both in research
               and teaching, has led to the establishment of the series Texts
               in Applied Mathematics (TAM).
               Thedevelopmentofnewcoursesisanaturalconsequenceofahighlevelof
               excitement on the research frontier as newer techniques, such as
               numerical and symbolic computer systems, dynamical systems, and
               chaos, mix with and reinforce the traditional methods of applied
               mathematics. Thus, the purpose of this textbook series is to
               meet the current and future needs of these advances and to
               encourage the teaching of new courses. TAM will publish
               textbooks suitable for use in advanced undergraduate and
               beginning graduate courses, and will complement the Applied Ma-
               ematical Sciences (AMS) series, which will focus on advanced
               textbooks and research-level monographs. Pasadena, California
               J.E. Marsden New York, New York L. Sirovich College Park,
               Maryland S.S. Antman Preface This book is based on a
               two-semester course in ordinary di?erential eq- tions that I
               have taught to graduate students for two decades at the U-
               versity of Missouri. The scope of the narrative evolved over
               time from an embryonic collection of supplementary notes,
               through many classroom tested revisions, to a treatment of the
               subject that is suitable for a year (or more) of graduate study.",
  publisher = "Springer Science \& Business Media",
  month     =  sep,
  year      =  2006,
  language  = "en",
  isbn      = "9780387357942"
}

@INPROCEEDINGS{Linderman2017,
  title     = "{Bayesian Learning and Inference in Recurrent Switching Linear
               Dynamical Systems}",
  booktitle = "Proceedings of the 20th International Conference on Artificial
               Intelligence and Statistics",
  author    = "Linderman, Scott and Johnson, Matthew and Miller, Andrew and
               Adams, Ryan and Blei, David and Paninski, Liam",
  editor    = "Singh, Aarti and Zhu, Jerry",
  abstract  = "Many natural systems, such as neurons firing in the brain or
               basketball teams traversing a court, give rise to time series
               data with complex, nonlinear dynamics. We can gain insight into
               these systems by decomposing the data into segments that are
               each explained by simpler dynamic units. Building on switching
               linear dynamical systems (SLDS), we develop a model class and
               Bayesian inference algorithms that not only discover these
               dynamical units but also, by learning how transition
               probabilities depend on observations or continuous latent
               states, explain their switching behavior. Our key innovation is
               to design these recurrent SLDS models to enable recent
               P{\'o}lya-gamma auxiliary variable techniques and thus make
               approximate Bayesian learning and inference in these models
               easy, fast, and scalable.",
  publisher = "PMLR",
  volume    =  54,
  pages     = "914--922",
  series    = "Proceedings of Machine Learning Research",
  year      =  2017,
  address   = "Fort Lauderdale, FL, USA"
}

@InProceedings{Nassar2018b,
  author        = {Josue Nassar and Scott W. Linderman and Monica Bugallo and Il Memming Park},
  title         = {Tree-Structured Recurrent Switching Linear Dynamical Systems for Multi-Scale Modeling},
  booktitle     = {International Conference on Learning Representations (ICLR)},
  year          = {2019},
  month         = nov,
  abstract      = {Many real-world systems studied are governed by complex, nonlinear dynamics. By modeling these dynamics, we can gain insight into how these systems work, make predictions about how they will behave, and develop strategies for controlling them. While there are many methods for modeling nonlinear dynamical systems, existing techniques face a trade off between offering interpretable descriptions and making accurate predictions. Here, we develop a class of models that aims to achieve both simultaneously, smoothly interpolating between simple descriptions and more complex, yet also more accurate models. Our probabilistic model achieves this multi-scale property through a hierarchy of locally linear dynamics that jointly approximate global nonlinear dynamics. We call it the tree-structured recurrent switching linear dynamical system. To fit this model, we present a fully-Bayesian sampling procedure using Polya-Gamma data augmentation to allow for fast and conjugate Gibbs sampling. Through a variety of synthetic and real examples, we show how these models outperform existing methods in both interpretability and predictive capability.},
  archiveprefix = {arXiv},
  day           = {30},
  eprint        = {1811.12386},
  keywords      = {neural-dynamics, statistical-neuroscience},
  code          = {https://github.com/catniplab/tree_structured_rslds},
}


@InProceedings{Zhao2016d,
  author        = {Zhao, Yuan and Park, Il Memming},
  title         = {Interpretable Nonlinear Dynamic Modeling of Neural Trajectories},
  booktitle     = {Advances in Neural Information Processing Systems (NIPS)},
  year          = {2016},
  abstract      = {A central challenge in neuroscience is understanding how neural system
implements computation through its dynamics. We propose a nonlinear
time series model aimed at characterizing interpretable dynamics
from neural trajectories. Our model assumes low-dimensional continuous
dynamics in a finite volume. It incorporates a prior assumption about
globally contractional dynamics to avoid overly enthusiastic extrapolation
outside of the support of observed trajectories. We show that our
model can recover qualitative features of the phase portrait such
as attractors, slow points, and bifurcations, while also producing
reliable long-term future predictions in a variety of dynamical models
and in real neural data.},
  archiveprefix = {arXiv},
  eprint        = {1608.06546},
  keywords      = {autoregressive, bifurcation, chaos, continuous-attractor, dynamics, neural-dynamics, nips, oscillation, tensorflow},
  primaryclass  = {q-bio.QM},
  youtube	= {https://www.youtube.com/watch?v=7oWRZRpaq_I},
}

@ARTICLE{Ganguli2008,
  title     = "Memory traces in dynamical systems",
  author    = "Ganguli, Surya and Huh, Dongsung and Sompolinsky, Haim",
  abstract  = "To perform nontrivial, real-time computations on a sensory input
               stream, biological systems must retain a short-term memory trace
               of their recent inputs. It has been proposed that generic
               high-dimensional dynamical systems could retain a memory trace
               for past inputs in their current state. This raises important
               questions about the fundamental limits of such memory traces and
               the properties required of dynamical systems to achieve these
               limits. We address these issues by applying Fisher information
               theory to dynamical systems driven by time-dependent signals
               corrupted by noise. We introduce the Fisher Memory Curve (FMC)
               as a measure of the signal-to-noise ratio (SNR) embedded in the
               dynamical state relative to the input SNR. The integrated FMC
               indicates the total memory capacity. We apply this theory to
               linear neuronal networks and show that the capacity of networks
               with normal connectivity matrices is exactly 1 and that of any
               network of N neurons is, at most, N. A nonnormal network
               achieving this bound is subject to stringent design constraints:
               It must have a hidden feedforward architecture that
               superlinearly amplifies its input for a time of order N, and the
               input connectivity must optimally match this architecture. The
               memory capacity of networks subject to saturating nonlinearities
               is further limited, and cannot exceed . This limit can be
               realized by feedforward structures with divergent fan out that
               distributes the signal across neurons, thereby avoiding
               saturation. We illustrate the generality of the theory by
               showing that memory in fluid systems can be sustained by
               transient nonnormal amplification due to convective instability
               or the onset of turbulence.",
  journal   = "Proceedings of the National Academy of Sciences",
  publisher = "National Academy of Sciences",
  volume    =  105,
  number    =  48,
  pages     = "18970--18975",
  month     =  dec,
  year      =  2008,
  keywords  = "dynamical-system, temporal-memory",
  issn      = "1091-6490",
  pmid      = "19020074",
  doi       = "10.1073/pnas.0804451105",
  pmc       = "PMC2596211"
}

@incollection{Zhang2013a,
    title = {Non-Uniform Camera Shake Removal Using a Spatially-Adaptive Sparse Penalty},
    author = {Zhang, Haichao and Wipf, David},
    booktitle = {Advances in Neural Information Processing Systems 26},
    editor = {C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
    pages = {1556--1564},
    year = {2013},
}

@ARTICLE{Goldman2009,
  title     = "Memory without feedback in a neural network",
  author    = "Goldman, Mark S",
  abstract  = "Memory storage on short timescales is thought to be maintained
               by neuronal activity that persists after the remembered stimulus
               is removed. Although previous work suggested that positive
               feedback is necessary to maintain persistent activity, here it
               is demonstrated how neuronal responses can instead be maintained
               by a purely feedforward mechanism in which activity is passed
               sequentially through a chain of network states. This feedforward
               form of memory storage is shown to occur both in architecturally
               feedforward networks and in recurrent networks that nevertheless
               function in a feedforward manner. The networks can be tuned to
               be perfect integrators of their inputs or to reproduce the
               time-varying firing patterns observed during some working memory
               tasks but not easily reproduced by feedback-based attractor
               models. This work illustrates a mechanism for maintaining
               short-term memory in which both feedforward and feedback
               processes interact to govern network behavior.",
  journal   = "Neuron",
  publisher = "Cell Press,",
  volume    =  61,
  number    =  4,
  pages     = "621--634",
  month     =  feb,
  year      =  2009,
  keywords  = "dynamical-system, feed-forward, memory",
  language  = "en",
  issn      = "0896-6273, 1097-4199",
  pmid      = "19249281",
  doi       = "10.1016/j.neuron.2008.12.012",
  pmc       = "PMC2674525"
}

@ARTICLE{Druckmann2012,
  title    = "Neuronal circuits underlying persistent representations despite
              time varying activity",
  author   = "Druckmann, Shaul and Chklovskii, Dmitri B",
  abstract = "BACKGROUND: Our brains are capable of remarkably stable stimulus
              representations despite time-varying neural activity. For
              instance, during delay periods in working memory tasks, while
              stimuli are represented in working memory, neurons in the
              prefrontal cortex, thought to support the memory representation,
              exhibit time-varying neuronal activity. Since neuronal activity
              encodes the stimulus, its time-varying dynamics appears to be
              paradoxical and incompatible with stable network stimulus
              representations. Indeed, this finding raises a fundamental
              question: can stable representations only be encoded with stable
              neural activity, or, its corollary, is every change in activity a
              sign of change in stimulus representation? RESULTS: Here we
              explain how different time-varying representations offered by
              individual neurons can be woven together to form a coherent,
              time-invariant, representation. Motivated by two ubiquitous
              features of the neocortex-redundancy of neural representation and
              sparse intracortical connections-we derive a network architecture
              that resolves the apparent contradiction between representation
              stability and changing neural activity. Unexpectedly, this
              network architecture exhibits many structural properties that
              have been measured in cortical sensory areas. In particular, we
              can account for few-neuron motifs, synapse weight distribution,
              and the relations between neuronal functional properties and
              connection probability. CONCLUSIONS: We show that the intuition
              regarding network stimulus representation, typically derived from
              considering single neurons, may be misleading and that
              time-varying activity of distributed representation in cortical
              circuits does not necessarily imply that the network explicitly
              encodes time-varying properties.",
  journal  = "Current biology: CB",
  volume   =  22,
  number   =  22,
  pages    = "2095--2103",
  month    =  nov,
  year     =  2012,
  language = "en",
  issn     = "0960-9822, 1879-0445",
  pmid     = "23084992",
  doi      = "10.1016/j.cub.2012.08.058",
  pmc      = "PMC3543774"
}

@misc{brockett2015finite,
  title={Finite Dimensional Linear Systems},
  author={Brockett, Roger W},
  year={2015},
  publisher={SIAM}
}


@article{hirsch1974differential,
  title={Differential equations, dynamical systems, and linear algebra (Pure and Applied Mathematics, Vol. 60)},
  author={Hirsch, Morris and Smale, Stephen},
  year={1974}
}


@book{Scholkopf2002,
  title     = "Learning with kernels : support vector machines, regularization,
               optimization, and beyond",
  author    = "Sch{\"o}lkopf, Bernhard and Smola, Alexander J",
  publisher = "MIT Press",
  series    = "Adaptive computation and machine learning",
  year      =  2002,
  url       = "http://www.worldcat.org/oclc/48970254",
  keywords  = "gaussian-process, kernel-design, kernel-method,
               machine-learning, rkhs"
}

@ARTICLE{Aronszajn1950,
  title   = "Theory of Reproducing Kernels",
  author  = "Aronszajn, Nachman",
  journal = "Transactions of the American Mathematical Society",
  volume  =  68,
  number  =  3,
  pages   = "337--404",
  year    =  1950,
  issn    = "0002-9947"
}

@Article{Park2012a,
  author    = {Il Memming Park and Sohan Seth and Murali Rao and Jos\'e C. Pr\'incipe},
  journal   = {Neural Computation},
  title     = {Strictly positive definite spike train kernels for point process divergences},
  year      = {2012},
  month     = aug,
  number    = {8},
  pages     = {2223--2250},
  volume    = {24},
  abstract  = {Exploratory tools that are sensitive to arbitrary statistical variations in spike train observations open up the possibility of novel neuroscientific discoveries. Developing such tools, however, is difficult due to the lack of Euclidean structure of the spike train space, and an experimenter usually prefers simpler tools that capture only limited statistical features of the spike train, such as mean spike count or mean firing rate. We explore strictly positive-definite kernels on the space of spike trains to offer both a structural representation of this space and a platform for developing statistical measures that explore features beyond count or rate. We apply these kernels to construct measures of divergence between two point processes and use them for hypothesis testing, that is, to observe if two sets of spike trains originate from the same underlying probability law. Although there exist positive-definite spike train kernels in the literature, we establish that these kernels are not strictly definite and thus do not induce measures of divergence. We discuss the properties of both of these existing nonstrict kernels and the novel strict kernels in terms of their computational complexity, choice of free parameters, and performance on both synthetic and real data through kernel principal component analysis and hypothesis testing.},
  doi       = {10.1162/NECO_a_00309},
  issue     = {8},
  pdf       = {Park2012a_spd_spike_train_kernel_divergence_color.pdf},
  owner     = {memming},
  timestamp = {2010.02.20},
}

@ARTICLE{Park2013a,
  author = {Park, Il Memming and Seth, Sohan and Paiva, Antonio R. C. and Li,
	Lin and Principe, Jose C.},
  title = {Kernel methods on spike train space for neuroscience: a tutorial},
  journal = {IEEE Signal Processing Magazine},
  year = {2013},
  volume = {30},
  pages = {149--160},
  number = {4},
  month = jul,
  abstract = {Over the last decade several positive definite kernels have been proposed
	to treat spike trains as objects in Hilbert space. However, for the
	most part, such attempts still remain a mere curiosity for both computational
	neuroscientists and signal processing experts. This tutorial illustrates
	why kernel methods can, and have already started to, change the way
	spike trains are analyzed and processed. The presentation incorporates
	simple mathematical analogies and convincing practical examples in
	an attempt to show the yet unexplored potential of positive definite
	functions to quantify point processes. It also provides a detailed
	overview of the current state of the art and future challenges with
	the hope of engaging the readers in active participation.},
  day = {24},
  doi = {10.1109/msp.2013.2251072},
  eprint = {1302.5964},
  issn = {1053-5888}
}


% vim: paste
