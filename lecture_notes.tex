\documentclass[a4paper,11pt]{exam}

\usepackage[T1]{fontenc}
\usepackage[top=2cm, bottom=2cm, left=3cm, right=3cm]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{bm} % bold greek
\usepackage{tcolorbox}
\usepackage{xcolor}
\usepackage{forloop}
\usepackage{amsbsy}
%\usepackage[framed,numbered]{matlab-prettifier}
%\usepackage{filecontents}
\usepackage[numbers,sort&compress,comma]{natbib}

\usepackage{graphicx}
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.mps,.eps,.ps}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

\newcommand*{\clapp}[1]{\hbox to 0pt{\hss#1\hss}}
\newcommand*{\mat}[1]{\boldsymbol{\mathrm{#1}}}
\newcommand*{\subdims}[3]{\clapp{\raisebox{#1}[0pt][0pt]{$\scriptstyle(#2 \times #3)$}}}
\fboxrule=1pt

\newcommand{\defvec}[1]{\expandafter\newcommand\csname v#1\endcsname{{\mathbf{#1}}}}
\newcommand{\defrv}[1]{\expandafter\newcommand\csname rv#1\endcsname{{\mathbf{#1}}}}
\newcounter{ct}
\forLoop{1}{26}{ct}{
    \edef\letter{\alph{ct}}
    \expandafter\defvec\letter
}
% distinguish random vector
\forLoop{1}{26}{ct}{
    \edef\letter{\Alph{ct}}
    \expandafter\defrv\letter
}
\forLoop{1}{26}{ct}{
    \edef\letter{\alph{ct}}
    \expandafter\defrv\letter
}
\newcommand{\tvv}[1]{\mathbf{\tilde{#1}}} % tilde vectors
\newcommand{\hvv}[1]{\mathbf{\hat{#1}}} % hat vectors

\newcommand{\braket}[2]{\ensuremath{\langle#1\mid#2\rangle}}
\newcommand{\bra}[1]{\ensuremath{\langle#1|}}
\newcommand{\ket}[1]{\ensuremath{|#1\rangle}}
\newcommand{\horzbar}{\rule[.5ex]{1.5em}{0.4pt}}

\newcommand{\inv}{^{-1}}
\newcommand{\trp}{{^\top}} % transpose
\newcommand{\ctrp}{{^\dagger}} % transpose
\newcommand{\tp}[1]{\ensuremath{{#1}^\top}} % transpose
\newcommand{\dm}[1]{\ensuremath{\mathrm{d}{#1}}} % dx dy dz dmu
\newcommand{\RN}[2]{\frac{\dm{#1}}{\dm{#2}}} % (Radon-Nikodym) derivative
\newcommand{\PD}[2]{\frac{\partial #1}{\partial #2}} % partial derivative
\newcommand{\norm}[1]{\ensuremath{\Vert{#1}\Vert}}
\providecommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\field}[1]{\ensuremath{\mathbb{#1}}}
\newcommand{\Kfield}{\field{K}}
\newcommand{\reals}{\field{R}}
\newcommand{\complex}{\field{C}}
\newcommand{\integers}{\field{Z}}
\newcommand{\naturalNumbers}{\field{N}}

\DeclareMathOperator*{\E}{\mathbb{E}} % expectation
\DeclareMathOperator*{\var}{var}
\DeclareMathOperator*{\cov}{cov}
\DeclareMathOperator*{\diag}{diag}
\newcommand{\system}[2]{\mathcal{#1}\left[ #2 \right]}
\DeclareMathOperator*{\tr}{tr} % trace
\newcommand{\indicator}[1]{\mathbf{1}_{#1}} % indicator function
\newcommand{\onehalf}{\frac{1}{2}}

\newcommand{\iid}{{\it i.i.d.}\xspace}
\newcommand{\iidsample}{\stackrel{iid}{\sim}}
\newcommand{\normalDist}{\mathcal{N}}
\newcounter{homework}
\newcommand{\homework}{\stepcounter{homework}\textcolor{violet}{\textbf{Homework \thehomework:}~}}
\newcommand{\funfact}{\textbf{Fun Fact:}~}

\pagestyle{headandfoot}
\runningheadrule
\runningheader{Nov 2022}{Linear Dynamics}{Il Memming Park}

\title{Lecture notes on linear dynamical systems for neuroscience}
\author{Il Memming Park, Matthew Dowling, Ayesha Vermani, \'Abel S\'agodi}
\begin{document}
\maketitle
\section{Motivation}
Our aim is to understand complex spatio-temporal phenomena that involves the nervous system.
In the physical space, there are many neurons, multiple subregions of a single neuron, multiple brain areas, and even multiple brains that are socially interacting.
Various measurement technologies allow access to many pixels/voxels imaged, many electrodes that record electrical activities, and many magnetic field sensors.
In the physical time, neural activity of interest in neuroscience span time scales from microseconds to years.
Only through time, the neural systems can process information, store information, adapt to new context, learn, and generate meaningful behavior.
Both for the practical data analysis, and for the theoretical study of neural system, we often face the grand challenge of making sense of high-dimensional spatio-temporal dynamics.
If we want to ``understand'' how the brain works, we need some intuition on high-dimensional spatio-temporal dynamics.

This may seem daunting at first, but we can stand on the shoulders of giants who developed various conceptual tools.
In addition, fortunately, any one can learn the mathematical intuition for making precise statements on those concepts.
In this week, we will learn versatile and extremely useful framework of linear dynamical systems.
There are two key intuitions covered within two subjects in mathematics.
\begin{tcolorbox}[colback=black!1!,title=Learning Objective 1]
    Intuition on high-dimensional spaces can be gained by first studying \textbf{linear algebra}.
\end{tcolorbox}
\begin{tcolorbox}[colback=black!1!,title=Learning Objective 2]
    Intuition on lawful temporal changes can be gained by first studying \textbf{linear dynamical systems}.
\end{tcolorbox}

Learning them benefits you because of their immense practicality.
For example, data analysis methods such as principal components analysis (PCA), filtering, fourier transform, and linear regression are much faster than corresponding nonlinear extensions.
The recent boost in deep neural networks is fueled by the fast linear operations implemented on modern computer architectures (GPGPU-computing).
\begin{tcolorbox}[colback=black!1!,title=Saves you time and the planet]
    Linear systems are computationally \emph{fast} and can be made numerically stable.
\end{tcolorbox}
As we will see, with linear dynamical systems theory, we can understand when systems shows stable behavior, and how quickly it forgets the past.
It may be surprising that there are a finite categories of linear dynamical systems if we are only concerned with their asymptotic behavior.
\begin{tcolorbox}[colback=black!1!,title=Easy asymptotic theory]
    We can understand \emph{all} possible (finite-dimensional) linear dynamical systems.
\end{tcolorbox}
This allows us to theoretically understand roughly what is going to happen in systems at longer time scales thanks to powerful first-order approximation techniques.
\begin{tcolorbox}[colback=black!1!,title=Linearize (locally)]
    Even complex nonlinear systems have a linear component.
\end{tcolorbox}
Of course, there are exceptions and limitations, however, there are extensions and generalizations that typically incorporates some form of linearity.

\section{Discrete-time Linear Dynamical Systems}
We will first dive into discrete-time systems.

\subsection{Memory traces in dynamical systems}
Consider a simple leaky membrane potential model
\begin{align}\label{eq:dtlds:LIF:def}
	v(t+1) &= a \cdot v(t) + I(t)
\end{align}
where $v(t) \in \reals$ is the membrane potential, $a \in \reals$ is a constant, and $I(t)$ is the externally current entering the neuron.

\begin{questions}
\question Given an initial condition $v(0)$ and the external current $I(t)$ for $t = 0, 1, \ldots $, write the general form of $v(t)$ using the summation notation (e.g. $\sum_{s=0}^t$). (Hint: try writing out $v(1), v(2), v(3)$ first and generalize.)
\vspace{\stretch{0.3}}
\question Sketch the solution over time from $t=0$ to $t=10$ for $a = 0.9$, $a = 1$, and $a = 2$ given $v(0) = 0$, $I(2) = 1$, and $I(t) = 0$ otherwise. Which one resembles the behavior of a neuronal membrane?
\vspace{\stretch{0.5}}
\question What is the value of $a$ for an input pulse would decay to $10\%$ of it original magnitude in $10$ time steps?
\vspace{\stretch{0.2}}

\newpage
As we shall see later when we go to continuous time, $0 < a < 1$ is tightly connected to the time constant.
The membrane potential time constant of a neuron is not very flexible; typically in the order of $10$ milliseconds.
Having many independent neurons does not help with the situation of forgetful membrane.
But maybe we can create a network of (non-spiking) neurons!

Consider a network of $n$ linear neurons where each neuron's activity (analogous to membrane potential) $x_i(t)$ evolves over time as,
\begin{align}\label{eq:dtlds:LDS}
	x_i(t+1) &= w_{i,i} x_i(t) + \sum_{j} w_{i,j} x_j(t) + b_i I(t)
\end{align}
where $w_{i,i}$ plays the role of $a$ in \eqref{eq:dtlds:LIF:def}, and $w_{i,j}$ is the influence of $j$-th neuron's membrane potential directly onto $i$-th neuron's membrane potential.
Note that there is no synaptic dynamics in this super simplified model.
Despite its simplicity, it is a very useful model of neural dynamics~\citep{Druckmann2012,Ganguli2008,Goldman2009}.

Let's write this is matrix form. Define the vector $\vx(t)$ and the matrix $W$:
\begin{align}
	\vx(t) &= 
	    \begin{bmatrix}
		x_1(t)\\
		x_2(t)\\
		\vdots\\
		x_n(t)
	    \end{bmatrix}
	\qquad
	W =
	    \begin{bmatrix}
		w_{1,1} & w_{1,2} & \cdots & w_{1,n}
		\\
		w_{2,1} & w_{2,2} & \cdots & w_{2,n}
		\\
		\vdots & \vdots & \ddots & \vdots
		\\
		w_{n,1} & w_{n,2} & \cdots & w_{n,n}
	    \end{bmatrix}
\end{align}
Note that $W \in \reals^{n \times n}$ is a square matrix.

\question Write the matrix form difference equation for \eqref{eq:dtlds:LDS}.
\vspace{\stretch{0.3}}
\question Write out the general form for $\vx(t)$ using matrix power.
\vspace{\stretch{0.3}}

Sure, these are solutions, but we need more insights.
First, let us recall some linear algebra to help us.
\begin{tcolorbox}[colback=black!1!,title=Matrix multiplication]
Let $A$ be an $n \times k$ matrix, and $B$ be an $k \times m$ matrix.
The $i$-th row and $j$-th column of $A$ is denoted $A_{i,j}$.
The matrix product $C = AB$ is $n \times m$, and its entries are given by,
\begin{align}
    \framebox[1.5cm]{\clapp{\raisebox{0pt}[1.5cm][1.5cm]{$\mat C$}}\subdims{-2.0cm} n m} =
    \framebox[1.0cm]{\clapp{\raisebox{0pt}[1.5cm][1.5cm]{$\mat A$}}\subdims{-2.0cm} n k} \ 
    \framebox[1.5cm]{\clapp{\raisebox{0mm}[4.5mm][2.5mm]{$\mat B$}}       \subdims{-8mm} k m}
    \qquad
    C_{i,j} = \left(
    A
    B
    \right)_{i,j}
    =
    \sum_k A_{i,k} B_{k,j}
    \label{eq:matmult}
\end{align}
\\[3mm]
\noindent
Note that matrix products don't commute, i.e., $AB \neq BA$.
\end{tcolorbox}
\newpage
But what does \emph{matrix multiplication} do?
Practicing with some special matrices will gives us an intuition.

\textbf{Diagonal matrices} are non-zero only on the diagonal entries.
\begin{align}\label{eq:LA:diag}
    \diag(\lambda_1, \lambda_2, \ldots, \lambda_n)
    =
    \begin{bmatrix}
	\lambda_1 & 0 & \cdots & 0 \\
	0 & \lambda_2 & \cdots & 0 \\
	\vdots & & \ddots & \vdots \\
	%0 & \cdots & \lambda_{n-1} & 0\\
	0 & \cdots & 0 & \lambda_n
    \end{bmatrix}
\end{align}

\question If $W = \diag(\lambda_1, \ldots, \lambda_n)$, what is the general form for $\vx(t)$?
\vspace{\stretch{0.3}}

\textbf{Identity matrices} are \textbf{diagonal matrices} with ones on the diagonal (zero otherwise):
\begin{align}\label{eq:LA:diag:eg}
    I &=
	\diag(1, 1, \ldots, 1)
    =
    \begin{bmatrix}
	1 & & \\
	& \ddots & \\
	& & 1
    \end{bmatrix}
\end{align}

\question If $W = I$, what is the general form for $\vx(t)$?
\vspace{\stretch{0.3}}

\textbf{Cross diagonal matrices} are non-zero only on the anti-diagonal entries.
\begin{align}\label{eq:LA:cdiag}
    C
    =
    \begin{bmatrix}
	0 & \cdots & 0 & c_1\\
	\vdots & & \ddots & \vdots \\
	%0 & \cdots & c_{n-1} & 0\\
	0 & c_{n-1} & \cdots & 0 \\
	c_n & 0 & \cdots & 0 \\
    \end{bmatrix}
\end{align}

\question What is the general form for $\vx(t)$ for the following cross-diagonal weight matrix?
\begin{align}\label{eq:LA:cdiag:eg}
    W
    =
    \begin{bmatrix}
	0 & 0 & 1 \\
	0 & 1 & 0\\
	1 & 0 & 0
    \end{bmatrix}
\end{align}
\vspace{\stretch{0.6}}

\textbf{Strictly upper triangular matrices} have the following zero entry pattern ($X$ marks arbitrary value, example only shown in $3 \times 3$):
\begin{align}\label{eq:LA:suppertri}
    \begin{bmatrix}
	0 & X & X \\
	0 & 0 & X\\
	0 & 0 & 0
    \end{bmatrix}
\end{align}

\question What is the general form for $\vx(t)$ for the following strictly upper triangular weight matrix?
\begin{align}\label{eq:LA:suppertri:eg}
    W
    =
    \begin{bmatrix}
	0 & 1 & 1 \\
	0 & 0 & 1\\
	0 & 0 & 0
    \end{bmatrix}
\end{align}
\vspace{\stretch{0.6}}

\newpage
\begin{tcolorbox}
    \textbf{Transpose} of a matrix (or vector) is defined as $(A\trp)_{i,j} = A_{j,i}$.
\end{tcolorbox}
\begin{tcolorbox}
    Two vectors $\vu$ and $\vv$ are \textbf{orthogonal} if $\vu\trp \vv = 0$.
\end{tcolorbox}
\begin{tcolorbox}
    $\vv \in \reals^{n \times 1}$ is a \text{unit vector} if $\norm{\vv} = \sqrt{\vv\trp \vv} = 1$, i.e., it is of unit length.
\end{tcolorbox}

Let the collection of vectors $(\vu_1, \vu_2, \cdots, \vu_n)$ form an \textit{orthonormal basis} of $\reals^n$, that is,
(1) any vector $\vx \in \reals^{n \times 1}$ can be expressed as $\vx = \sum_{i=1}^n \alpha_i \vu_i$ where $\alpha_i$ are scalar,
(2) if $i \neq j$, then $\vu_i$ and $\vu_j$ are orthogonal,
and
(3) $\vu_i$ is a unit vector.

An \textbf{orthonormal matrix} is a real-valued square matrix where the collection of column vectors form an orthonormal basis.
\begin{align}\label{eq:LA:orthonormal}
U &=
    \begin{bmatrix}
	\kern.3em\vline\kern.3em
	&
	&
	\kern.3em\vline\kern.3em
	\\
	\vu_1 & \cdots & \vu_n \\
	\kern.3em\vline\kern.3em
	&
	&
	\kern.3em\vline\kern.3em
    \end{bmatrix}
\end{align}

\question Show that $U \in \reals^{n \times n}$ is orthonormal if and only if $U U\trp = I$.
\vspace{\stretch{0.5}}

A complex valued square matrix $U \in \complex^{n \times n}$ is \textbf{unitary}, if $U U\ctrp = I$ where $\ctrp$ denotes conjugate transpose operation.

\question Show that multiplying unitary matrix to a column vector preserves its norm, i.e., it is an isometric transformation.
\vspace{\stretch{0.5}}

\question In the absence of input, i.e., $I(t) = 0$, what can you say about $\vx(t)$ given $\vx(0)$? (geometrically)
\vspace{\stretch{0.3}}

A \textbf{permutation matrix} is a matrix where each row and column has a single unity and zero otherwise.

\question Show that a permutation matrix is a unitary matrix.
\vspace{\stretch{0.5}}

\clearpage
\question The following \textbf{rotation matrix} is a unitary matrix.
\begin{align}\label{eq:LA:rotation}
    W &=
    \begin{bmatrix}
	\cos(\theta) & -\sin(\theta)\\
	\sin(\theta) & cos(\theta)
    \end{bmatrix}
\end{align}
\begin{parts}
\part Sketch out the general form of $\vx(t)$ when $I(t) = 0$ and $\theta = \pi/4$.
\vspace{\stretch{0.3}}
\part How is the dynamics qualitatively different when $\theta$ is a rational multiple of $\pi$ or not?
\vspace{\stretch{0.2}}
\end{parts}

\question Orthonormal matrices can also ``flip'' or reflect vectors with respect to axes. Take the following $W$.
\begin{align}\label{eq:LA:flip}
    W &=
    \begin{bmatrix}
	-1 & 0\\
	 0 & 1
    \end{bmatrix}
\end{align}
\begin{parts}
\part Show that $W$ is unitary.
\vspace{\stretch{0.1}}
\part What is the general form of $\vx(t)$ when $I(t) = 0$?
\vspace{\stretch{0.2}}
\end{parts}

A matrix is \textbf{symmetric} if $A = A\trp$. A complex matrix is \textbf{Hermitian} (or self-adjoint) if $A = A\ctrp$.

\begin{proposition}
Any \textit{real symmetric square matrix} $A$ can be \textbf{eigendecomposed} into the following form,
\begin{align}\label{eq:eig}
    A = U \Lambda U\trp \qquad \text{(eigen-decomposition)}
\end{align}
where $U = \begin{bmatrix}\vu_1 & \cdots & \vu_n \end{bmatrix}$
is an orthonormal matrix, and $\Lambda$ is a real-valued diagonal matrix.
\end{proposition}

\question Let the connectivity matrix $W$ be real symmetric. Use the eigendecomposition \eqref{eq:eig} to answer the following questions.
\begin{parts}
\part What is $WW$?
\vspace{\stretch{0.2}}
\part What is $W^n$?
\vspace{\stretch{0.2}}
\part What is the general form of $\vx(t)$ in the absence of input ($I(t) = 0$)?
\vspace{\stretch{0.5}}
\part What is the general form of $\vx(t)$ with input?
\vspace{\stretch{0.5}}
\end{parts}

This is a.k.a.~diagonalization.

\clearpage

\begin{tcolorbox}
\funfact Diagonal matrices commute, i.e., $IA = AI = A$ for any matrix $A$.
\end{tcolorbox}


\question
    \begin{parts}
	\part Show that $\Sigma \vu_i = \lambda_i \vu_i$, that is, columns of $U$ correspond to right eigenvectors with corresponding eigenvalues as the diagonal elements of $\Lambda$.
	    \vspace{\stretch{1}}
	\part Order the eigenvalues in decreasing order $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0$.
	Show that $\lambda_1$ corresponds to the effective ``time constant'' of the network.
	    \vspace{\stretch{3}}
    \end{parts}

\begin{tcolorbox}
    \funfact if $\vu$ and $\vv$ are orthogonal, $\vu\trp \vv = 0$.
\end{tcolorbox}

\begin{tcolorbox}\funfact $\vv \in \reals^{n \times 1}$ is a unit vector if $\norm{\vv} = \sqrt{\vv\trp \vv} = 1$, i.e., it is of unit length.\end{tcolorbox}
\begin{parts}
    \part What's $V\trp V$?
    \vspace{\stretch{1}}
    \part
    What's $V V\trp$?
    \begin{tcolorbox}
	\funfact For square matrices $A$ and $B$, if $AB=I$ then $BA=I$.
	$B$ is the \textit{matrix inverse} of $A$, that is, $B = A\inv$.
	(They are inverses of each other.)
    \end{tcolorbox}
    \vspace{\stretch{1}}
    \part Let $\vw = V\trp \vu$. Show that $\vw$ is a representation of $\vu$ in a new basis system.
    In other words, as a linear combination of the column vectors of $V$, or equivalently $V \vw = \vu$.
    %Let $\vv$ be a unit vector.
%Inner product of an arbitrary vector $\vu \in \reals^{n \times 1}$ with $\vv$ is the length of the projection of $\vu$ to $\vv$:
%Let $V$ be an orthonormal basis matrix, that is, $V\trp V = I$.
\vspace{\stretch{1}}
    \begin{tcolorbox}\funfact
	Given two vectors $\vu$ and $\vv$, their inner product is,
    \begin{align}\label{eq:proj1}
	\vu\trp \vv = \norm{\vu} \cdot \norm{\vv} \cos(\theta)
    \end{align}
    where $\theta$ is the angle between the two vectors.
    \end{tcolorbox}
\end{parts}

\question \textbf{Every linear transformation is a matrix}

\question \textbf{Similarity transform and change of basis}
Let $M$ be any invertible matrix.
Consider a linear system over discrete time, $x_{t+1} = Ax_t$.
A change of variable $y_t = Mx_t$ is a linear trnasformation, or a change of basis.
Show that $B = M\inv A M$ is the new dynamics matrix for $y_t$.
($B$ is said to be \textit{``similar''} to $A$)
\vspace{\stretch{0.5}}

\question Consider a square matrix $A$. Let $B = M\inv A M$ be a similarity transform of $A$.
Show that similar matrices have the same eigenvalues.
\vspace{\stretch{1}}

If A is not a square matrix, eigendecomposition is not defined. 

\begin{proposition}
    More generally, any rectangular matrix $A \in \complex^{m \times n}$ can be factored into the following form,
    \begin{align}\label{eq:eig}
        A = U \Sigma V\trp \qquad \text{(singular value decomposition)}
    \end{align}
    where $U\in \complex^{m \times m}$ is a unitary matrix, $\Sigma$ is a real-valued diagonal matrix and
    $V\in \complex^{n \times n}$ is a unitary matrix.
    .
\end{proposition}

The diagonal entries in $\Sigma$ are called the \textbf{singular values} of A.

\section{Linear Algebra}
\begin{tcolorbox}[colback=black!1!,title=Matrix Fun Facts]
\textbf{Matrix multiplication}:
\noindent
Note that matrix products don't commute, i.e., $AB \neq BA$.\\[1mm]
Vector inner product and outer products are special cases:
%Let $\vu$ and $\vv$ be column vectors,
\begin{align}
    \vu\trp \vv &=
    \begin{bmatrix}
	u_1, u_2, \cdots, u_n
    \end{bmatrix}
    \begin{bmatrix}
	v_1\\v_2\\\vdots\\ v_n
    \end{bmatrix}
    =
    \sum_k u_k v_k
    \,&
    \text{\emph{inner product}}
    \\
    \vu \vv\trp &=
    \begin{bmatrix}
	u_1\\u_2\\\vdots\\ u_n
    \end{bmatrix}
    \begin{bmatrix}
	v_1, v_2, \cdots, v_m
    \end{bmatrix}
    =
    \begin{bmatrix}
	u_1 v_1 & u_1 v_2 & \cdots & u_1 v_m
	\\
	u_2 v_1 & u_2 v_2 & \cdots & u_2 v_m
	\\
	\vdots & \vdots & \ddots & \vdots
	\\
	u_n v_1 & u_n v_2 & \cdots & u_n v_m
    \end{bmatrix}
    \,&
    \text{\emph{outer product}}
\end{align}
Matrix outer product stacks:
\begin{align}
    \begin{bmatrix}
	\kern.3em\vline\kern.3em \\
	\vu_1 \\
	\kern.3em\vline\kern.3em
    \end{bmatrix}
    \begin{bmatrix}
	\horzbar \vv_1 \horzbar
    \end{bmatrix}
    +
    \begin{bmatrix}
	\kern.3em\vline\kern.3em \\
	\vu_2 \\
	\kern.3em\vline\kern.3em
    \end{bmatrix}
    \begin{bmatrix}
	\horzbar \vv_2 \horzbar
    \end{bmatrix}
    =
    \begin{bmatrix}
	\kern.3em\vline\kern.3em
	&
	\kern.3em\vline\kern.3em
	\\
	\vu_1 & \vu_2 \\
	\kern.3em\vline\kern.3em
	&
	\kern.3em\vline\kern.3em
    \end{bmatrix}
    \begin{bmatrix}
	\horzbar \vv_1 \horzbar \\
	\horzbar \vv_2 \horzbar
    \end{bmatrix}
\end{align}
%
Matrix times diagonal matrix scales column vectors:
\begin{align}
    \begin{bmatrix}
	\kern.3em\vline\kern.3em
	&
	&
	\kern.3em\vline\kern.3em
	\\
	\vu_1 & \cdots & \vu_n \\
	\kern.3em\vline\kern.3em
	&
	&
	\kern.3em\vline\kern.3em
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
	d_1 & & \\
	    & \ddots & \\
	    & & d_n
    \end{bmatrix}
    &=
    \begin{bmatrix}
	\kern.3em\vline\kern.3em
	&
	&
	\kern.3em\vline\kern.3em
	\\
	d_1 \vu_1 & \cdots & d_n \vu_n \\
	\kern.3em\vline\kern.3em
	&
	&
	\kern.3em\vline\kern.3em
    \end{bmatrix}
    \\
    \begin{bmatrix}
	d_1 & & \\
	    & \ddots & \\
	    & & d_n
    \end{bmatrix}
    \cdot
    \begin{bmatrix}
	\horzbar \vv_1 \horzbar \\
	\vdots \\
	\horzbar \vv_n \horzbar
    \end{bmatrix}
    &=
    \begin{bmatrix}
	\horzbar d_1 \vv_1 \horzbar \\
	\vdots \\
	\horzbar d_n \vv_n \horzbar
    \end{bmatrix}
\end{align}
\end{tcolorbox}

\question \textbf{Upper triangular matrix:}


Some further fun factoids for later.
\subsection{Hadamard inequality}
For any $n \times n$ positive definite matrix $A$,
\begin{align}
    \det{A} \leq A_{11} A_{22} \cdots A_{nn}
\end{align}
For any real-valued matrix $A$,
\begin{align}
    A = 
    \begin{bmatrix}
	\kern.3em\vline\kern.3em
	&
	&
	\kern.3em\vline\kern.3em
	\\
	\vu_1 & \cdots & \vu_n \\
	\kern.3em\vline\kern.3em
	&
	&
	\kern.3em\vline\kern.3em
    \end{bmatrix}
    \qquad
    \abs{\det{A}} \leq \prod_{i=1}^n \norm{\vu_i}
\end{align}
This is a useful inequality for approximating marginal log-probabilities of multivariate Gaussians~\citep{Zhang2013a}.
\subsection{Householder transform}
Let $\vx \neq \vy \in \reals^n$ with $\norm{\vx} = \norm{\vy}$.
There exists a vector $\vu \in \reals^n$ and $\norm{\vu} = 1$ such that, $P\vx = \vy$,
where the Householder transform $P$ is defined by $P = (I - 2\vu\tp{\vu})$.
The vector $\vu$ is uniquely provided up to sign by,
\[
    \vu = \pm \frac{x-y}{\norm{x-y}}.
\]
Note that $P^2 = I - 4\vu\tp{\vu} + 4\vu\tp{\vu}\vu\tp{\vu} = I$.
Hence, $P = P\inv$, $Py = x$. Also, $P = \tp{P}$.

\subsection{QR-decomposition}
QR-decomposition is Gram-Schmidt orthogonalization on all columns of a matrix.
Implementation using Householder transformation is numerically more stable.

Let $A \in \reals^{m \times n}$, $m \geq n$.
\[
    A = QR,
\]
where $Q \in \reals{m \times n}$ is an orthogonal matrix with $\tp{Q}Q = I_n$,
and $R \in \reals^{n \times n}$ is an upper triangular matrix.

\subsection{Positive definite, positive semi-definite}
\begin{proposition}
    A matrix $A$ is positive semi-definite if and only if there is a positive semi-definite matrix square root, $B^2 = A$. This square root is unique.
\end{proposition}

\begin{theorem}[via Gabriel Peyr\'e's tweet]
    Given two positive-definite matrices $A, B$. The unique solution of
    \[
	XAX=B
    \]
    for a positive definite $X$ is given by,
    \[
	X = A^{-\onehalf} \left( A^\onehalf B A^\onehalf \right)^\onehalf A^{-\onehalf}
    \]
\end{theorem}
Proof is trivial. Useful for pushing forward Gaussian distribution with covariance $A$ to $B$.

\subsection{Reading assignment}
Read~\citep{Goldman2009} with the following questions in mind:

\question What is the form of recurrent weight matrix corresponding to the sequential activation network in Fig 1? Consider the problem in discrete time~\eqref{eq:dtlds:LDS}.
\vspace{\stretch{0.5}}

\question Take a unitary matrix $U$ and left multiply to the weight matrix obtained from above. Can you rewrite the dynamics equation in a new coordiate system $\vy(t) = U\vx(t)$? How does this relate to Fig 3C?
\vspace{\stretch{0.5}}


\subsection{Non-normal dynamical systems}

\section{Continuous-time Linear Dynamical Systems}
\subsection{Companion form matrices}
Consider an $N\text{-th}$ order polynomial of the form
\begin{align}
    p(x) = x^N + a_{N-1} x^{N-1} + \cdots + a_1 x + a_0
\end{align}
then, the matrix $F$ defined below is called a companion form matrix
\begin{align}
    F = \begin{pmatrix} 0 & 1 & 0 & \cdots & 0\\
                        0 & 0 & 1 & \cdots & 0\\
                        \vdots & \vdots & \vdots & \ddots & \vdots\\
                        -a_0 & -a_1 & -a_2 & \cdots & -a_{N-1}\end{pmatrix}
\end{align}
and we have that the eigenvalues of $F$ coincide with the roots of the characteristic polynomial defined by $p(x)$.  That is, if $\det(\lambda I - F) = 0$, then $p(\lambda) = 0$.  
\subsection{Linear differential equations}
We begin by examining $N-\text{th}$ order linear differential equations with constant coefficients
\begin{align}
    \dot{x}(t) &= A x(t)\\
    x(0) &= x_0
\end{align}
If $A$ is not in companion form, then we can always reason about the surrogate system
\begin{align}
    \dot{y}(t) &= F y(t)\\
    y(0) &= y_0
\end{align}
where $F$ is now a companion form matrix, that is similar to $A$.  The exact coordinate transformation can be found by taking the Jordan decomposition of $A$ and $F$
\begin{align}
    A &= T^{-1} J T\\
    F &= S^{-1} J S\\
\end{align}
then setting $x(t) = T^{-1} S y(t)$.
\subsection{Characteristic and minimal polynomials}
We define the characteristic polynomial 
\begin{align}
    p_{A}(x) = \det \left( x I - A \right)
\end{align}
and the minimal polynomial, $\chi_A(x)$, to be the lowest order polynomial which satisfies $\chi_A(A) = 0$.

\section{Nonlinear systems that are really linear}
\subsection{Linearization theorems}
Let us consider a (time-invariant and autonomous) nonlinear dynamical system:
\begin{align}
    \dot{x} = f(x)
\end{align}
where $x \in \reals^n$ and $f: \reals^n \to \reals^n$.
Much of this nonlinear dynamical system can be understood piece by piece using our knowledge of linear dynamical systems.

\begin{tcolorbox}[colback=black!1!,title=Linearize that beast!]
    Most nonlinear dynamical systems are locally a linear dynamical system, but strange non-local things can happen.
\end{tcolorbox}

In particular, the dynamics around equilibria (fixed points) are topologically conjugate to that of the linearization.
Let $\bar{x}$ be a fixed point of the dynamics that is, $f(\bar{x}) = 0$.
Note that there are potentially many or infinitely many fixed points.
Let $[Df]_{i,j}(\bar{x}) = \frac{\partial f_i}{\partial x_j}\bigr\rvert_{\bar{x}}$ be the Jacobian (matrix), i.e., linearization of the dynamics at $\bar{x}$:
\begin{align}\label{eq:linearized_around_FP}
    \dot{x} = Df(\bar{x}) (x - \bar{x})
\end{align}


\begin{theorem}[Hartman-Grobman]
    If $Df(\bar{x})$ has no zero or purely imaginary eigenvalues then there is a homeomorphism $h$ defined on some neighborhood $U$ of $\bar{x}$ in $\reals^n$ locally taking obits of the nonlinear flow $\phi_t$ to those of the linear flow.
\end{theorem}
As the theorem states, linearlization around the fixed point fails if there are eigenvalues with zero real part.
They are typically rare in practice, but of great interest to nonlinear dynamics analysis and important for bifurcation theory.
Fixed points that allows linearlization are called \textbf{hyperbolic} fixed points.
For hyperbolic fixed points, the stability of nonlinear system is equivalent to that of the linearized system.

Away from the fixed points, the flow of nonlinear dynamical system is just topologically conujugate to a boring constant flow:
\begin{theorem}[Rectification]
    If $p \in \reals^n$ and $f(p) \neq 0$, then there are open sets $U, V \in \reals^n$ with $p \in U$ and a diffiomorphism $g: U \to V$ such that the differential equation in the new coordinates, that is, the differential equation,
    $$ \dot{y} = Dg(g\inv(y)) f(g\inv(y)), $$
is given by $(\dot{y}_1, \ldots, \dot{y}_n) = (1, 0, 0, \ldots, 0)$.
\end{theorem}
See \citet[Lemma 1.120]{Chicone2006}.

Of course, a patchwork of linear dynamics can create some interesting systems.
In neural data analysis, locally linear dynamical system model~\cite{Zhao2016d} and recurrent switching linear dynamical systems type models utilize this~\citep{Linderman2017,Nassar2018b}.

\begin{equation}\label{eq:cubicode}
\dot x = x^3 + y\\
\dot y = 3x \\
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figs/linearization.pdf}
    \caption{Linearization of the system described in Equation \ref{eq:cubicode}.}
    \label{fig:linearization}
\end{figure}

\subsection{Kernel methods}
When linear methods fail due to nonlinearity, you can always try linear methods applied to a nonlinearly mapped space.

All you need is an inner product to implement many linear methods (least squares regression, PCA, CCA, k-means).

\begin{tcolorbox}[colback=black!1!,title=Kernel Trick]
You can implicitly map your data to an infinitely high-dimensional space by using a (positive semi-definite) \textbf{kernel}.
\end{tcolorbox}

\subsection{Koopman operators}

\end{questions}

\bibliographystyle{plainnat}
\bibliography{refs}
\end{document}
